CrayPat/X:  Version 24.03.0 Revision 1705b3005 rhel8.8_x86_64  02/13/24 19:56:47

Number of PEs (MPI ranks):   16
                           
Numbers of PEs per Node:      4  PEs on each of  4  Nodes
                           
Numbers of Threads per PE:    1
                           
Number of Cores per Socket:  24

Execution start time:  Wed May 22 23:00:51 2024

System name and speed:  rzadams1097  3.700 GHz (nominal)

AMD                        CPU  Family: 25  Model: 144  Stepping:  1

Core Performance Boost:  All 16 PEs have CPB capability

Current path to data file:
  /usr/workspace/srinathv/REPOS/mpitutorial/tutorials/mpi-reduce-and-allreduce/code/./reduce_int64_t_stddev+3372019-669309878t   (RTS, 16 data files)


Notes for table 1:

  This table shows functions that have significant exclusive time,
    averaged across ranks.
  For further explanation, see the "General table notes" below, or 
    use:  pat_report -v -O profile ...

Table 1:  Profile by Function Group and Function

  Time% |       Time | Imb. Time |  Imb. |     Calls | Group
        |            |           | Time% |           |  Function
        |            |           |       |           |   PE=HIDE
       
 100.0% | 762.397447 |        -- |    -- | 300,008.0 | Total
|--------------------------------------------------------------------
|  96.2% | 733.612910 | 21.495102 |  3.0% |       1.0 | USER
||-------------------------------------------------------------------
||  96.2% | 733.612910 | 21.495102 |  3.0% |       1.0 | main
||===================================================================
|   3.8% |  28.783865 |        -- |    -- | 300,007.0 | MPI
||-------------------------------------------------------------------
||   3.4% |  26.302010 |  5.482412 | 18.4% | 100,001.0 | MPI_Allreduce
||   0.2% |   1.901096 |  0.520602 | 22.9% | 100,001.0 | MPI_Barrier
||   0.1% |   0.580755 |  1.130191 | 70.5% | 100,001.0 | MPI_Reduce
||   0.0% |   0.000002 |  0.000002 | 50.5% |       1.0 | MPI_Finalize
||   0.0% |   0.000002 |  0.000000 | 14.0% |       1.0 | MPI_Init
||   0.0% |   0.000000 |  0.000000 | 38.6% |       1.0 | MPI_Comm_rank
||   0.0% |   0.000000 |  0.000000 | 10.4% |       1.0 | MPI_Comm_size
||===================================================================
|   0.0% |   0.000671 |  0.000298 | 32.8% |        -- | ETC
||-------------------------------------------------------------------
||   0.0% |   0.000671 |  0.000298 | 32.8% |        -- | _dl_start_user
|====================================================================

Notes for table 2:

  This table shows functions that have the most significant exclusive
    time, taking the maximum time across ranks and threads.
  For further explanation, see the "General table notes" below, or 
    use:  pat_report -v -O profile_max ...

Table 2:  Profile of maximum function times

  Time% |       Time | Imb. Time |  Imb. | Function
        |            |           | Time% |  PE=[max,min]
|--------------------------------------------------------
| 100.0% | 755.108013 | 21.495102 |  3.0% | main
||-------------------------------------------------------
|| 100.0% | 755.108013 |        -- |    -- | pe.12
||  96.5% | 728.405177 |        -- |    -- | pe.0
||=======================================================
|   4.2% |  31.784423 |  5.482412 | 18.4% | MPI_Allreduce
||-------------------------------------------------------
||   4.2% |  31.784423 |        -- |    -- | pe.0
||   0.6% |   4.725972 |        -- |    -- | pe.12
||=======================================================
|   0.3% |   2.421699 |  0.520602 | 22.9% | MPI_Barrier
||-------------------------------------------------------
||   0.3% |   2.421699 |        -- |    -- | pe.5
||   0.1% |   0.496534 |        -- |    -- | pe.0
||=======================================================
|   0.2% |   1.710946 |  1.130191 | 70.5% | MPI_Reduce
||-------------------------------------------------------
||   0.2% |   1.710946 |        -- |    -- | pe.0
||   0.0% |   0.136969 |        -- |    -- | pe.1
||=======================================================
|   0.0% |   0.000970 |  0.000298 | 32.8% | _dl_start_user
||-------------------------------------------------------
||   0.0% |   0.000970 |        -- |    -- | pe.6
||   0.0% |   0.000536 |        -- |    -- | pe.3
||=======================================================
|   0.0% |   0.000003 |  0.000002 | 50.5% | MPI_Finalize
||-------------------------------------------------------
||   0.0% |   0.000003 |        -- |    -- | pe.10
||   0.0% |   0.000000 |        -- |    -- | pe.3
||=======================================================
|   0.0% |   0.000002 |  0.000000 | 14.0% | MPI_Init
||-------------------------------------------------------
||   0.0% |   0.000002 |        -- |    -- | pe.14
||   0.0% |   0.000001 |        -- |    -- | pe.0
||=======================================================
|   0.0% |   0.000000 |  0.000000 | 38.6% | MPI_Comm_rank
||-------------------------------------------------------
||   0.0% |   0.000000 |        -- |    -- | pe.8
||   0.0% |   0.000000 |        -- |    -- | pe.5
||=======================================================
|   0.0% |   0.000000 |  0.000000 | 10.4% | MPI_Comm_size
||-------------------------------------------------------
||   0.0% |   0.000000 |        -- |    -- | pe.9
||   0.0% |   0.000000 |        -- |    -- | pe.8
|========================================================

Observation:  Metric-Based Rank Order

    No rank order was suggested based on the USER Time metric because
    that metric was already well balanced across the nodes.


Notes for table 3:

  This table shows the ranks with maximum, mean, and minimum time for
    functions with significant time, within the function groups. It
    also shows MPI message statistics for functions in the MPI group.
    Note that this table includes both point to point and  collective
    communications, using estimates for the latter based on a naive
    implementation using the former, and does not reflect
    optimizations by the MPI library.
  For further explanation, see the "General table notes" below, or 
    use:  pat_report -v -O load_balance_m ...

Table 3:  Load Balance with MPI Message Stats

  Time% |       Time |   MPI Msg |     MPI Msg |  Avg | Group
        |            |     Count |       Bytes |  MPI |  PE=[mmm]
        |            |           |             |  Msg | 
        |            |           |             | Size | 
       
 100.0% | 762.397447 | 200,002.0 | 1,200,012.0 | 6.00 | Total
|----------------------------------------------------------------
|  96.2% | 733.612910 |       0.0 |         0.0 |   -- | USER
||---------------------------------------------------------------
||  99.0% | 755.108013 |       0.0 |         0.0 |   -- | pe.12
||  95.8% | 730.718148 |       0.0 |         0.0 |   -- | pe.10
||  95.5% | 728.405177 |       0.0 |         0.0 |   -- | pe.0
||===============================================================
|   3.8% |  28.783865 | 200,002.0 | 1,200,012.0 | 6.00 | MPI
||---------------------------------------------------------------
||   4.5% |  33.991907 | 200,002.0 | 1,200,012.0 | 6.00 | pe.0
||   4.1% |  31.513643 | 200,002.0 | 1,200,012.0 | 6.00 | pe.1
||   1.0% |   7.288905 | 200,002.0 | 1,200,012.0 | 6.00 | pe.12
||===============================================================
|   0.0% |   0.000671 |       0.0 |         0.0 |   -- | ETC
||---------------------------------------------------------------
||   0.0% |   0.000970 |       0.0 |         0.0 |   -- | pe.6
||   0.0% |   0.000640 |       0.0 |         0.0 |   -- | pe.5
||   0.0% |   0.000536 |       0.0 |         0.0 |   -- | pe.3
|================================================================

Notes for table 4:

  This table shows the MPI library functions that are used to send a
    significant number of bytes, taking the average across sender
    ranks of the sum of bytes sent from the sender to all destination
    ranks. It also shows how many bytes are attributable to each of
    its call paths. It also shows a count of messages and the number
    of messages that fall into each bin of message sizes. For each
    path, it shows the ranks that send the minimum, mean, and maximum
    number of bytes.
    Note that this table includes both point to point and  collective
    communications, using estimates for the latter based on a naive
    implementation using the former, and does not reflect
    optimizations by the MPI library.
  For further explanation, see the "General table notes" below, or 
    use:  pat_report -v -O mpi_callers ...

Table 4:  MPI Message Stats by Caller

    MPI |     MPI Msg |   MPI Msg | MsgSz <16 | Function
    Msg |       Bytes |     Count |     Count |  Caller
 Bytes% |             |           |           |   PE=[mmm]
       
 100.0% | 1,200,012.0 | 200,002.0 | 200,002.0 | Total
|------------------------------------------------------------
|  66.7% |   800,008.0 | 100,001.0 | 100,001.0 | MPI_Allreduce
|        |             |           |           |  main
|||----------------------------------------------------------
3||  66.7% |   800,008.0 | 100,001.0 | 100,001.0 | pe.4
3||  66.7% |   800,008.0 | 100,001.0 | 100,001.0 | pe.0
3||  66.7% |   800,008.0 | 100,001.0 | 100,001.0 | pe.15
|||==========================================================
|  33.3% |   400,004.0 | 100,001.0 | 100,001.0 | MPI_Reduce
|        |             |           |           |  main
|||----------------------------------------------------------
3||  33.3% |   400,004.0 | 100,001.0 | 100,001.0 | pe.4
3||  33.3% |   400,004.0 | 100,001.0 | 100,001.0 | pe.0
3||  33.3% |   400,004.0 | 100,001.0 | 100,001.0 | pe.15
|============================================================

Notes for table 5:

  This table shows values shown for HiMem calculated from information
    in the /proc/self/numa_maps files captured near the end of the
    program. It is the total size of all pages, including huge pages,
    that were actually mapped into physical memory from both private
    and shared memory segments.
  For further explanation, see the "General table notes" below, or 
    use:  pat_report -v -O himem ...

Table 5:  Memory High Water Mark by Numa Node

   Process |     HiMem |     HiMem |     HiMem |     HiMem | Numanode
     HiMem | Numa Node | Numa Node | Numa Node | Numa Node |  PE=HIDE
 (MiBytes) |         0 |         1 |         2 |         3 | 
           | (MiBytes) | (MiBytes) | (MiBytes) | (MiBytes) | 
|----------------------------------------------------------------------
|     112.3 |     100.8 |       3.7 |       5.9 |       1.9 | numanode.0
|     105.8 |       8.2 |      89.9 |       5.8 |       1.9 | numanode.1
|     106.1 |       8.2 |       3.7 |      92.1 |       2.1 | numanode.2
|     105.6 |       8.1 |       3.5 |       5.6 |      88.4 | numanode.3
|======================================================================

Notes for table 6:

  This table shows total wall clock time for the ranks with the
    maximum, mean, and minimum time, as well as the average across
    ranks.
    It also shows maximum memory usage from /proc/self/numa_maps for
    those ranks, and on average.  The usage is total size of all
    pages, including huge pages, that were actually mapped into
    physical memory from both private and shared memory segments.
  For further explanation, see the "General table notes" below, or 
    use:  pat_report -v -O program_time ...

Table 6:  Wall Clock Time, Memory High Water Mark

    Process |   Process | PE=[mmm]
       Time |     HiMem | 
            | (MiBytes) | 
           
 762.496060 |     107.5 | Total
|---------------------------------
| 762.498895 |     106.1 | pe.2
| 762.495715 |     106.0 | pe.6
| 762.492744 |     112.3 | pe.8
|=================================

========================  Additional details  ========================

General table notes:

    The default notes for a table are based on the default definition of
    the table, and do not account for the effects of command-line options
    that may modify the content of the table.
    
    Detailed notes, produced by the pat_report -v option, do account for
    all command-line options, and also show how data is aggregated, and
    if the table content is limited by thresholds, rank selections, etc.
    
    An imbalance metric in a line is based on values in main threads
    across multiple ranks, or on values across all threads, as applicable.
    
    An imbalance percent in a line is relative to the maximum value
    for that line across ranks or threads, as applicable.
    
    If the number of Calls for a function is shown as "--", then that
    function was not traced and the other values in its line summarize
    the data collected for functions that it calls and that were traced.
    
Experiment:  trace

Original path to data file:
  /usr/WS1/srinathv/REPOS/mpitutorial/tutorials/mpi-reduce-and-allreduce/code/reduce_int64_t_stddev+3372019-669309878t/xf-files   (RTS, 16 data files)

Original program:  ./reduce_int64_t_stddev

Instrumented with:  pat_run -w -g mpi ./reduce_int64_t_stddev

Program invocation:  ./reduce_int64_t_stddev 1000000 100000

Exit Status:  0 for 16 PEs

Memory pagesize:  4 KiB

Memory hugepagesize:  Not Available

Programming environment:  CRAY

Runtime environment variables:
  CRAYPAT_LD_LIBRARY_PATH=/opt/cray/pe/perftools/24.03.0/lib64
  CRAYPAT_OPTS_EXECUTABLE=libexec64/opts
  CRAYPAT_ROOT=/opt/cray/pe/perftools/24.03.0
  CRAYPE_VERSION=2.7.31.11
  CRAY_BINUTILS_VERSION=/opt/cray/pe/cce/17.0.1
  CRAY_CC_VERSION=17.0.1
  CRAY_FTN_VERSION=17.0.1
  CRAY_LIBSCI_VERSION=24.03.0
  CRAY_MPICH_VERSION=8.1.29
  CRAY_PERFTOOLS_VERSION=24.03.0
  LMOD_FAMILY_COMPILER_VERSION=17.0.1
  LMOD_FAMILY_CRAYPE_CPU_VERSION=false
  LMOD_FAMILY_CRAYPE_NETWORK_VERSION=false
  LMOD_FAMILY_CRAYPE_VERSION=2.7.31.11
  LMOD_FAMILY_LIBSCI_VERSION=24.03.0
  LMOD_FAMILY_MPI_VERSION=8.1.29
  LMOD_FAMILY_PRGENV_VERSION=8.5.0
  LMOD_VERSION=8.7.37
  MPIBIND_TOPOFILE=/usr/tce/packages/mpibind/lstopo.rzadams.xml
  MPICH_DIR=/opt/cray/pe/mpich/8.1.29/ofi/crayclang/16.0
  MPICH_LOCAL_SPAWN_SERVER=0
  OMP_NUM_THREADS=24
  OMP_PLACES=threads
  OMP_PROC_BIND=spread
  PAT_RT_EXPERIMENT=trace
  PAT_RT_PERFCTR_DISABLE_COMPONENTS=nvml,rocm_smi
  PERFTOOLS_VERSION=24.03.0
  PMI_CONTROL_PORT=11997,11996
  PMI_FD=19
  PMI_RANK=4
  PMI_SHARED_SECRET=9530128412139840093
  PMI_SIZE=16

Report time environment variables:
    CRAYPAT_ROOT=/opt/cray/pe/perftools/24.03.0

Number of MPI control variables collected:  136

  (To see the list, specify: -s mpi_cvar=show)

Report command line options:  -T

Operating system:
  Linux 4.18.0-513.24.1.1toss.t4.x86_64 #1 SMP Thu Apr 11 17:08:33 PDT 2024

Estimated minimum instrumentation overhead per call of a traced function,
  which was subtracted from the data shown in this report
  (for raw data, use the option:  -s overhead=include):
    Time  0.044  microsecs

Number of traced functions that were called:  12

  (To see the list, specify:  -s traced_functions=show)

